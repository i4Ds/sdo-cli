{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a9ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunpy.visualization.colormaps import cm\n",
    "from torchvision.utils import make_grid\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os \n",
    "import sunpy\n",
    "from sunpy.visualization.colormaps import cm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms.functional as F\n",
    "from sdo.sood.data.sdo_ml_v2_dataset import get_default_transforms, SDOMLv2NumpyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#inspect an image\n",
    "\n",
    "#Channels that correspond to HMI Magnetograms \n",
    "HMI_WL = ['Bx','By','Bz']\n",
    "#A colormap for visualizing HMI\n",
    "HMI_CM = LinearSegmentedColormap.from_list(\"bwrblack\", [\"#0000ff\",\"#000000\",\"#ff0000\"])\n",
    "\n",
    "def channel_to_map(name):\n",
    "    \"\"\"Given channel name, return colormap\"\"\"\n",
    "    return HMI_CM if name in HMI_WL else cm.cmlist.get('sdoaia%d' % int(name))\n",
    "\n",
    "def vis(X, cm):\n",
    "    \"\"\"Given image, colormap, and visualize results\"\"\"\n",
    "    Xcv = cm(X)\n",
    "    return (Xcv[:,:,:3]*255).astype(np.uint8)\n",
    "\n",
    "def show_grid(imgs, ordered_dates, df, ncols=4, channel=\"171\"):\n",
    "    nrows=int(len(imgs)/ncols)\n",
    "    if nrows <= 0:\n",
    "        nrows = 1\n",
    "        ncols = len(imgs)\n",
    "    fix, axs = plt.subplots(figsize=(20,9), ncols=ncols, nrows=nrows, squeeze=True)\n",
    "    row_index = 0\n",
    "    i = 0\n",
    "    for t_obs in ordered_dates[:10]:\n",
    "        t_obs = t_obs.isoformat(timespec='milliseconds').replace(\"0+00:00\", \"Z\")\n",
    "        img = imgs[t_obs]\n",
    "        row = df.loc[t_obs]\n",
    "        col = i % ncols\n",
    "        if i != 0 and i % ncols == 0:\n",
    "            row_index = row_index + 1\n",
    "        axs[row_index, col].imshow(img)\n",
    "        img_name = f\"{t_obs} {channel}A\"\n",
    "        score = row[\"score_norm\"]\n",
    "        axs[row_index, col].set_title(f\"{img_name}\\n with score \" + \"%.5f\" % score)\n",
    "        axs[row_index, col].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        i += 1\n",
    "    plt.show()\n",
    "\n",
    "def visualize_batch(loader, ordered_dates, img_df):\n",
    "    for batch_idx, samples in enumerate(loader):\n",
    "        X, y = samples\n",
    "        V = {}\n",
    "        for x, t_obs in zip(X, y[\"T_OBS\"]):\n",
    "            x = x.permute(1,2,0) # torch to pillow\n",
    "            x = np.squeeze(x.numpy())\n",
    "            v = vis(x, channel_to_map(171))\n",
    "            V[t_obs] = Image.fromarray(v)\n",
    "        \n",
    "        show_grid(V, ordered_dates, df, ncols=5)\n",
    "        break   \n",
    "        \n",
    "        \n",
    "def visualize_batch_norm(loader, ordered_times, df):\n",
    "    for batch_idx, samples in enumerate(loader):\n",
    "        X, y = samples\n",
    "        V = {}\n",
    "        for x, t_obs in zip(X, y[\"T_OBS\"]):\n",
    "            grid = make_grid(x, normalize=True, value_range=(-1.0, 1.0))\n",
    "            ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(\n",
    "                    1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "            m = cm.cmlist.get('sdoaia%d' % int(171))\n",
    "            v = np.squeeze(ndarr[:, :, 0])\n",
    "            v = m(v)\n",
    "            v = (v[:, :, :3]*255).astype(np.uint8)\n",
    "            V[t_obs] = Image.fromarray(v)\n",
    "        show_grid(V, ordered_times, df, ncols=5)\n",
    "        break  \n",
    "        \n",
    "def anomaly_threshold(loader, ordered_times, df):\n",
    "    for batch_idx, samples in enumerate(loader):\n",
    "        X, y = samples\n",
    "        V = {}\n",
    "        for x, t_obs in zip(X, y[\"T_OBS\"]):\n",
    "            grid = make_grid(x, normalize=True)\n",
    "            ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(\n",
    "                        1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "            lower = ndarr.mean() - 2 * ndarr.std()\n",
    "            upper = ndarr.mean() + 2 * ndarr.std()\n",
    "            print(lower, upper)\n",
    "            \n",
    "            ndarr[ndarr < upper] = 0\n",
    "            #ndarr[ndarr >= upper] = 255\n",
    "            \n",
    "            ndarr = np.invert(ndarr)\n",
    "            m = cm.cmlist.get('sdoaia%d' % int(171))\n",
    "            v = np.squeeze(ndarr[:, :, 0])\n",
    "            v = m(v)\n",
    "            v = (v[:, :, :3]*255).astype(np.uint8)\n",
    "            V[t_obs] = Image.fromarray(ndarr)\n",
    "        show_grid(V, ordered_times, df, ncols=5)\n",
    "        break\n",
    "        \n",
    "def spaced_obs_times(df, min_diff_seconds=24*60*60, min_size = 100):\n",
    "    obs_times = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        t_obs = row[\"t_obs\"]\n",
    "        has_close_neighbour = False\n",
    "        for obs_time in obs_times:\n",
    "            diff = abs((t_obs - obs_time).total_seconds())\n",
    "            if diff < min_diff_seconds:\n",
    "                has_close_neighbour = True\n",
    "                #print(f\"ignoring {t_obs} for diff {diff}\")\n",
    "                break\n",
    "                \n",
    "            \n",
    "        if not has_close_neighbour:\n",
    "            score = row[\"score_norm\"]\n",
    "            #print(f\"found obs time {t_obs} with score {score}\")\n",
    "            obs_times.append(t_obs)\n",
    "            \n",
    "        if len(obs_times) >= min_size:\n",
    "            break\n",
    "\n",
    "    return obs_times\n",
    "\n",
    "def get_data_loader(obs_times):\n",
    "    storage_root = \"/home/marius/data/sdomlv2_full/sdomlv2.zarr\"\n",
    "    storage_driver = \"fs\"\n",
    "    cache_max_size = 1*1024*1024*2014\n",
    "    test_year = [\"2011\", \"2014\", \"2016\", \"2019\"]\n",
    "    channel= \"171A\"\n",
    "    target_size = 256\n",
    "    mask_limb = False\n",
    "    mask_limb_radius_scale_factor = 1.0\n",
    "    transforms = get_default_transforms(\n",
    "                target_size=target_size, channel=channel, mask_limb=mask_limb, radius_scale_factor=mask_limb_radius_scale_factor)\n",
    "    dataset = SDOMLv2NumpyDataset(\n",
    "                storage_root=storage_root,\n",
    "                storage_driver=storage_driver,\n",
    "                cache_max_size=cache_max_size,\n",
    "                year=test_year,\n",
    "                start=None,\n",
    "                end=None,\n",
    "                freq=None,\n",
    "                obs_times=obs_times[:10],\n",
    "                irradiance=None,\n",
    "                irradiance_channel=None,\n",
    "                goes_cache_dir=None,\n",
    "                channel=channel,\n",
    "                transforms=transforms,\n",
    "                reduce_memory=True\n",
    "            )\n",
    "    \n",
    "    print(f\"found dataset with size {len(dataset)}\")\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False,\n",
    "                          drop_last=False,\n",
    "                          prefetch_factor=2)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d03ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "#default-256 outputs\n",
    "sample_pred_path = \"/home/marius/sdo-cli/output/predictions/20220803-111810_cevae/predictions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a20a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sample_pred_path)\n",
    "\n",
    "df = df.sort_values(by=['score'], ascending=False)\n",
    "df[\"score_norm\"] = (df[\"score\"]-df[\"score\"].min())/(df[\"score\"].max()-df[\"score\"].min())\n",
    "folder_time_format = \"%Y%m%d-%H%M%S\"\n",
    "df[\"t_obs\"] = pd.to_datetime(df[\"t_obs\"])\n",
    "#somehow some images in 2019 are duplicates?\n",
    "df = df.drop_duplicates(subset=['t_obs'], keep='first')\n",
    "df = df.set_index('t_obs', drop=False, verify_integrity=True)\n",
    "\n",
    "top_obs_times = []\n",
    "\n",
    "for index, row in df[:1000].iterrows():\n",
    "    t_obs = row[\"t_obs\"] #Â .isoformat(timespec='milliseconds').replace(\"+00:00\", \"Z\") #.replace(microsecond=0)\n",
    "    top_obs_times.append(t_obs)\n",
    "\n",
    "spaced_top_obs_times_7d = spaced_obs_times(df, min_diff_seconds=24*60*60*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e8a281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2014-08-01 14:48:13.730000+0000', tz='UTC'),\n",
       " Timestamp('2014-01-07 10:12:13.730000+0000', tz='UTC'),\n",
       " Timestamp('2014-11-25 12:48:12.340000+0000', tz='UTC'),\n",
       " Timestamp('2014-10-30 04:42:13.470000+0000', tz='UTC'),\n",
       " Timestamp('2014-10-03 03:54:12.340000+0000', tz='UTC'),\n",
       " Timestamp('2014-12-21 13:00:13.740000+0000', tz='UTC'),\n",
       " Timestamp('2014-08-10 01:12:12.340000+0000', tz='UTC'),\n",
       " Timestamp('2014-07-11 03:00:13.360000+0000', tz='UTC'),\n",
       " Timestamp('2014-02-25 00:54:12.990000+0000', tz='UTC'),\n",
       " Timestamp('2014-12-08 19:12:12.340000+0000', tz='UTC')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaced_top_obs_times_7d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09801ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spaced_7d_top_loader = get_data_loader(spaced_top_obs_times_7d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcede208",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_batch_norm(spaced_7d_top_loader, spaced_top_obs_times_7d, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, samples in enumerate(spaced_7d_top_loader):\n",
    "    X, y = samples\n",
    "    V = {}\n",
    "    for x, t_obs in zip(X, y[\"T_OBS\"]):\n",
    "        grid = make_grid(x, normalize=True, value_range=(-1.0, 1.0))\n",
    "        ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(\n",
    "                    1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "        m = cm.cmlist.get('sdoaia%d' % int(171))\n",
    "        v = np.squeeze(ndarr[:, :, 0])\n",
    "        v = m(v)\n",
    "        v = (v[:, :, :3]*255).astype(np.uint8)\n",
    "        V[t_obs] = Image.fromarray(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img_path = \n",
    "aia_wave = \"171A\"\n",
    "\n",
    "src_images = list(Path(src_img_path).rglob(f'*__{aia_wave}_src.png'))\n",
    "for src_img in src_images:\n",
    "    header = get_meta_info(\n",
    "        src_img.name, src_img_path / Path(\"meta.csv\"))\n",
    "    loader = HEKEventManager(db_connection_string)\n",
    "    timestamp_str = src_img.name.split(\"__\")[0]\n",
    "    timestamp = dt.datetime.strptime(timestamp_str, date_format)\n",
    "    events_df = loader.find_events_at(\n",
    "        timestamp, observatory=\"SDO\", instrument=\"AIA\", event_types=hek_event_types)\n",
    "    if len(events_df) < 1:\n",
    "        logger.warn(\n",
    "            f\"no events found\")\n",
    "        continue\n",
    "    # filter events that were observed in the respective wavelength, possibly also filter by feature extraction method\n",
    "    events_df = events_df[events_df['obs_channelid'].str.contains(\n",
    "        \"171\")]\n",
    "    logger.info(\n",
    "        f\"after filter {len(events_df)} events\")\n",
    "    hek_bboxes, hek_polygons = convert_events_to_pixelunits(\n",
    "        events_df, header)\n",
    "    map_path = sood_map_path / Path(src_img.name)\n",
    "    anomaly_boxes = extract_bounding_boxes_from_anomaly_map(\n",
    "        map_path, mode=\"otsu\", scale=8, gaussian_filter=False)\n",
    "    save_fig_with_hek_bounding_boxes_and_anomalies(src_img, hek_bboxes, hek_polygons, anomaly_boxes, out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdo-cli",
   "language": "python",
   "name": "sdo-cli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
